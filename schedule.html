<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>RHOBIN Workshop@CVPR23</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR23'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
  </head>


  <body>
    <div id="header">
      <div id="logo">
	<h1>
		<center>
	    	 <span style="font-size:50%;color:#777;font-weight:normal">The first Workshop</span><br>
         Reconstruction of Human-Object Interactions (RHOBIN)<br>
  		</center>
	</h1><br>
</center>
</h1><br>
<div class="my-image-box-header">
 <center><img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168">
  <img src="data/Adobe_Corporate_logo.svg.png" width="320" height="83"> 

</center>
  
</div>
  <h2>
		<center>
      <span style="font-size:92%;color:#777;font-weight:normal">June 19 @
          <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> 
      <span style="font-size:92%;color:#777;font-weight:bold">Vancouver, Canada</span><br>
      <span style="font-size:130%;color:#777;font-weight:bold">East 18@Vancouver Convention Center</span>      

    </center>
	</h2><br>
	</div>
  
    <div id="menu">
    <center>
      <ul>
        <li class="first"><a href="./index.html" accesskey="1">Home</a></li>
      </ul>
    </center>
</div>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>


<div id="content">
  <div id="tz_select">
     <p><h2 style="color:rgb(188, 131, 9);">Vancouver Time (PST)</h2></p>
  </div>
<!-- <p>
<ul>
<li>
    Here is <a href="https://live.allintheloop.net/App/ortra/ortraECCV2022" target="_blank">the ECCV Platform</a>.
</li>
<li>
    Please log in using your ECCV credentials, click "Workshops", then "Oct 24", find our workshop and click "Join Now".
</li>
</ul>
</p> -->


<table class="schedule" id="program">
  <tr class="row_type_tt">
    <td width="80px">
	    <b style="color:white;">Start</b>
    </td>
    <td width="80px">
	   <b style="color:white;">End</b>
    </td>
    <td>
	    <b style="color:white;">Event</b>
    </td>
  </tr>
  
  <tr class="row_type_C">
    <td <span> 19 June 2023 08:20:00 PST </span></td>
    <td <span> 19 June 2023 08:30:00 PST </span></td>
    <td>
        <p class="poster_title">Welcome and Introductions</p>
        Workshop Organizers 
    </td>
  </tr>
  <tr class="row_type_B">
    <td <span> 19 June 2023 08:30:00 PST</span></td>
    <td <span> 19 June 2023 09:00:00 PST</span></td>
      <td>
          <table style="width:100%"><tr><td style="border:none;width:20%"><a href="https://people.ciirc.cvut.cz/~sivic" target="_blank"> <img alt src="data/josef_sivic.jpg" height="140"/></a>Josef Sivic<br>(CIIRC, CTU)</td>
              <td style="border:none;width:80%"><p class="poster_title">Learning manipulation skills from instructional videos</p>
                  <p class="poster_abstract">
                      People easily learn how to change a flat tire of a car or perform resuscitation by observing other people doing the same task, for example, in an instructional video. This involves advanced visual intelligence abilities such as interpreting sequences of human actions that manipulate objects to achieve a specific task. Currently, however, there is no artificial system with a similar level of cognitive visual competence. In this talk, I will describe our recent progress on learning from instructional videos. In the first part, I will focus on learning video and language representations from the input videos and transcribed narrations. In the second part, I will focus on learning from instructional videos about how people manipulate objects and demonstrate transferring the learned skill to a robotic manipulator.
                  </p>
              </td></tr></table>
      </td>

  </tr>
 

  <tr class="row_type_A">
    
    <td  <span> 19 June 2023 09:00:00 PST</span></td>
    <td  <span> 19 June 2023 09:30:00 PST</span></td>
      <td>
          <table style="width:100%"><tr><td style="border:none;width:20%"><a href="https://tsattler.github.io/" target="_blank"> <img alt src="data/Torsten_profile_small.jpg" height="140"/></a>Torsten Sattler<br>(CIIRC, CTU)</td>
              <td style="border:none;width:80%"><p class="poster_title">Recent advances in visual localization</p>
                  <p class="poster_abstract">
                      In order to take scene context into account when modelling human-object
                      interactions, it is important to know where in the scene the human is.
                      One way of obtaining this information is via visual localization. Visual
                      localization is the problem of estimating the position and orientation
                      of a camera with respect to some (3D) representation of the scene.
                      Localization plays an important role in many applications of computer
                      vision, including augmented / mixed / virtual reality (and thus the
                      meta-verse), and autonomous robots such as self-driving cars and drones.
                      Starting by motivating the use of visual localization for human-object
                      interactions, we will discuss recent advances in visual localization.
                      The talk will focus on two main questions:
                      Assuming that (part of) the visual localization process will happen via a service in the cloud
                      (e.g., using services provided by companies such as Microsoft, Google,
                      or Niantic), what are the implications on privacy of using such a
                      service? How well do current state-of-the-art algorithms solve the
                      visual localization problem (in terms of localization accuracy, memory
                      consumption, and run-time)?
                  </p>
              </td></tr></table>
      </td>
  </tr>
 
 <tr class="row_type_B">
    <td  <span> 19 June 2023 09:30:00 PST</span></td>
    <td  <span> 19 June 2023 10:00:00 PST</span></td>
    <td>
        <table style="width:100%"><tr><td style="border:none;width:20%"><a href="" target="_blank"> <img alt src="data/" height="140"/></a></td>
                <td style="border:none;width:80%"><p class="poster_title">Presentations of the Rhobin Challenge results given by the winners<br></td></tr></table>
    </td>
  </tr>

  
 <tr class="row_type_F">
    <td <span> 19 June 2023 10:00:00 PST</span></td>
    <td <span> 19 June 2023 10:30:00 PST</span></td>
    <td>
        <table style="width:100%"><tr><td style="border:none;width:20%"><a href="" target="_blank"> <img alt src="data/" height="140"/></a></td>
                <td style="border:none;width:80%"><p class="poster_title">Coffee Break & Posters<br></td></tr></table>
    </td>
  </tr>

 <tr class="row_type_B">
    <td <span> 19 June 2023 10:30:00 PST</span></td>
    <td <span> 19 June 2023 11:00:00 PST</span></td>
    <td>
      <table style="width:100%"><tr><td style="border:none;width:20%"><a href="https://www.cs.utexas.edu/users/grauman/" target="_blank"> <img alt src="data/grauman.jpg" height="140"/></a>Kristen Grauman<br>(University of Texas Austin and
          Meta AI Research)</td>
        <td style="border:none;width:80%"><p class="poster_title">Human-object interaction in first-person video</p>
          <p class="poster_abstract">
            
        </p>
          </td></tr></table>

    </td>
  </tr>

  <tr class="row_type_A">
    <td <span> 19 June 2023 11:00:00 PST</span></td>
    <td <span> 19 June 2023 11:30:00 PST</span></td>
    <td>
        <table style="width:100%"><tr><td style="border:none;width:20%"><a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html" target="_blank"> <img alt src="data/siyu.jpeg" height="140"/></a>Siyu Tang <br>(ETH, Zurich)</td>
        <td style="border:none;width:80%"><p class="poster_title">
          Synthesizing human motions in 3D scenes.
        </p>
        <p class="poster_abstract">

        Abstract: Simulating human behavior and interactions within various environments is crucial for numerous applications, including generating training data for machine learning algorithms, creating autonomous agents for interactive applications like augmented and virtual reality (AR/VR) or computer games, guiding architectural design decisions, and more. In this talk, I will discuss our previous and ongoing research efforts dedicated to modeling and synthesizing digital humans, with the ultimate goal of enabling them to exhibit spontaneous behavior and move autonomously in a digital environment. A key aspect of our work, which I will highlight during the talk, is the development of the Guided Motion Diffusion model. This approach generates high-quality and diverse human motion based on textual prompts and spatial constraints, such as motion trajectories and obstacles. Through a detailed exploration of our research, I will illustrate how these techniques can be applied to various scenarios, ultimately enriching and enhancing the realism of digital human behaviors across multiple domains.
      </p>

        </td></tr></table>
    </td>
  </tr> 
  <tr class="row_type_B">
    <td <span> 19 June 2023 11:30:00 PST</span></td>
    <td <span> 19 June 2023 12:00:00 PST</span></td>
    <td>
      <table style="width:100%"><tr><td style="border:none;width:20%"><a href="http://persci.mit.edu/people/adelson" target="_blank"> <img alt src="data/ted-adelson.png" height="140"/></a>Edward H. Adelson<br>(MIT)</td>
        <td style="border:none;width:80%"><p class="poster_title">  The rich world of camera-based tactile sensing
 </p>
          <p class="poster_abstract">
            There is a growing interest in the use of camera-based tactile sensors such as the GelSight sensors we are developing in my laboratory.
            A small internal camera looks through a clear gel at an opaque skin.
            Computer vision is used to measure the shape of the skin as it deforms due to contact with objects in the world.
            It is possible to capture the full x,y,z deformation of all the points in the contact patch, and this gives a complete picture of the mechanical interaction between the sensor and the world.
            From this you can infer many things, such as object shape, object pose, force, torque, slip, and material properties such as hardness or roughness.
            The tactile information can also be used in a feedback loop for reactive control of robotic tasks.
          </p>
          </td></tr></table>
            
        </td></tr></table>
    </td>
  </tr>
 
  <tr class="row_type_B">
    <!-- <td></td> <span> 19 June 2023<br> 12:00:00 PST </span></td> -->
    <td>
      <table style="width:100%"><tr><td style="border:none;width:20%"></td>
        <td style="border:none;width:80%"><p class="poster_title">
      <span>Accepted Papers Talks</span>
    </td>
  </tr>
</table>

<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>

<h2 id="contact">Acknowledgements</h2>
<p>Website template borrowed from: 
<a href="https://futurecv.github.io/" target="_blank">https://futurecv.github.io/</a>
(Thanks to <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>)
</p>

<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>
</html>
